name: Scrape Events
on:
  schedule:
    - cron: '0 * * * *' # Run every hour (00:00, 01:00, etc. UTC; 07:00, 08:00, etc. WIB)
  workflow_dispatch: # Allow manual trigger
jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.9'
          
      - name: Set up Chrome
        uses: browser-actions/setup-chrome@v1
        with:
          chrome-version: 'latest'
          
      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install selenium webdriver-manager requests beautifulsoup4
          
      - name: Run Scrape Script
        run: |
          echo "Running scrape script..."
          cd scraper
          python scrape_events.py
          cd ..
          
      - name: Debug - Show current directory
        run: pwd && ls -la
        
      - name: Debug - Show event.json
        run: cat event.json || echo "No event.json found"
        
      - name: Upload Debug HTML
        uses: actions/upload-artifact@v4
        with:
          name: debug-artifacts
          path: |
            debug.html
            event.json
          retention-days: 5
          
      - name: Check for Changes and Commit
        run: |
          echo "Checking for changes in event.json..."
          if git diff --quiet event.json; then
            echo "No changes in event.json, skipping commit"
          else
            echo "Changes detected, committing..."
            git config --global user.name "GitHub Action"
            git config --global user.email "action@github.com"
            git add event.json
            git commit -m "Update event.json with latest scraped events"
            git push
          fi
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}